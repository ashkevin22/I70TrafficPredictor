{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I70 Traffic Predictor\n",
    "---\n",
    "\n",
    "### The goal of this project is to be able to predict the amount of cars that will pass by a traffic measuring station along I70 for a given hour of a day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using data from NOAA (National Oceanic and Atmospheric Administration) for daily snow totals and daily snow depth from a measuring station in Winter Park, Colorado, as well as data from CDOT (Colorado Department of Transportation) with a counter of how many cars pass by the station (count station 000120 on I70 right before Idaho Springs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import data\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MONTH           int64\n",
       "YEAR            int64\n",
       "DAY             int64\n",
       "HOUR            int64\n",
       "SNOW_DEPTH    float64\n",
       "DAILY_SNOW    float64\n",
       "COUNT           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_data_one_year(traffic_data, snow_depth_data, daily_snow_data):\n",
    "    '''\n",
    "    Function to return a transformed dataframe of a year's worth of data\n",
    "    :param traffic_data: The traffic dataframe for a given year, exported from CDOT\n",
    "    :param snow_depth_data: The snow depth dataframe for the same year, from NOAA\n",
    "    :param daily_snow_data: The daily snow datafram fro the same year, from NOAA\n",
    "    :return: A single, formatted dataframe containing all of the information from the parameters\n",
    "    '''\n",
    "    # Replace all invalid data with 0s\n",
    "    daily_snow_data.replace(\"T\", 0, inplace=True)\n",
    "    daily_snow_data.replace(\"T\", 0, inplace=True)\n",
    "    daily_snow_data.replace(\"M\", 0, inplace=True)\n",
    "    daily_snow_data.replace(\"M\", 0, inplace=True)\n",
    "    # Only care about secondary direction\n",
    "    # drop rows where COUNTDIR != 'S'\n",
    "    traffic_data = traffic_data[traffic_data['COUNTDIR'] == 'S']\n",
    "    df = pd.DataFrame()\n",
    "    # temp arrays so we can store them in the datafram\n",
    "    temp_month = []\n",
    "    temp_day = []\n",
    "    temp_year = []\n",
    "    temp_hour = []\n",
    "    temp_snow_depth = []\n",
    "    temp_daily_snow = []\n",
    "    temp_count = []\n",
    "    \n",
    "    # iterate through every hour of every day and get the data for it\n",
    "    for i, date in enumerate(traffic_data['FormattedDate']):\n",
    "        for j in range(24):\n",
    "            temp_count.append(traffic_data.iloc[i]['HOUR' + str(j)])\n",
    "            temp_hour.append(j)\n",
    "            month = datetime.strptime(date, \"%m/%d/%Y\").month\n",
    "            day = datetime.strptime(date, \"%m/%d/%Y\").day\n",
    "            year = datetime.strptime(date, \"%m/%d/%Y\").year\n",
    "            temp_month.append(month)\n",
    "            temp_day.append(day)\n",
    "            temp_year.append(year)\n",
    "            temp_snow_depth.append(float(snow_depth_data.iloc[day-1][calendar.month_abbr[month]]))\n",
    "            temp_daily_snow.append(float(daily_snow_data.iloc[day-1][calendar.month_abbr[month]]))\n",
    "            \n",
    "    # finally create the dataframe\n",
    "    df['MONTH'] = temp_month\n",
    "    df['YEAR'] = temp_year\n",
    "    df['DAY'] = temp_day\n",
    "    df['HOUR'] = temp_hour\n",
    "    df['SNOW_DEPTH'] = temp_snow_depth\n",
    "    df['DAILY_SNOW'] = temp_daily_snow\n",
    "    df['COUNT'] = temp_count\n",
    "\n",
    "    return df\n",
    "    \n",
    "# Create the datafram for 2019\n",
    "snow_depth19 = pd.read_csv(\"data/2019WinterParkSnowDepth.csv\")\n",
    "daily_snow19 = pd.read_csv(\"data/2019WinterParkDailySnow.csv\")\n",
    "traffic19 = pd.read_csv(\"data/AnnualTrafficVolume2019.csv\")\n",
    "df1 = transform_data_one_year(traffic19, snow_depth19, daily_snow19)\n",
    "\n",
    "# Create the datafram for 2020\n",
    "snow_depth20 = pd.read_csv(\"data/2020WinterParkSnowDepth.csv\")\n",
    "daily_snow20 = pd.read_csv(\"data/2020WinterParkDailySnow.csv\")\n",
    "traffic20 = pd.read_csv(\"data/AnnualTrafficVolume2020.csv\")\n",
    "\n",
    "# Create the datafram for 2021\n",
    "df2 = transform_data_one_year(traffic20, snow_depth20, daily_snow20)\n",
    "snow_depth21 = pd.read_csv(\"data/2021WinterParkSnowDepth.csv\")\n",
    "daily_snow21 = pd.read_csv(\"data/2021WinterParkDailySnow.csv\")\n",
    "traffic21 = pd.read_csv(\"data/AnnualTrafficVolume2021.csv\")\n",
    "df3 = transform_data_one_year(traffic21, snow_depth21, daily_snow21)\n",
    "\n",
    "# Create the datafram for 2022\n",
    "snow_depth22 = pd.read_csv(\"data/2022WinterParkSnowDepth.csv\")\n",
    "daily_snow22 = pd.read_csv(\"data/2022WinterParkDailySnow.csv\")\n",
    "traffic22 = pd.read_csv(\"data/AnnualTrafficVolume2022.csv\")\n",
    "df4 = transform_data_one_year(traffic22, snow_depth22, daily_snow22)\n",
    "\n",
    "# concatenate the dataframes\n",
    "frames = [df1, df2, df3, df4]\n",
    "full_dataframe = pd.concat(frames)\n",
    "full_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1699: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 2s 4ms/step - loss: 1.4312 - mean_squared_logarithmic_error: 1.4312 - val_loss: 0.9502 - val_mean_squared_logarithmic_error: 0.9502\n",
      "Epoch 2/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.8430 - mean_squared_logarithmic_error: 0.8429 - val_loss: 0.3792 - val_mean_squared_logarithmic_error: 0.3792\n",
      "Epoch 3/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.2933 - mean_squared_logarithmic_error: 0.2933 - val_loss: 0.1777 - val_mean_squared_logarithmic_error: 0.1777\n",
      "Epoch 4/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.2274 - mean_squared_logarithmic_error: 0.2274 - val_loss: 0.1772 - val_mean_squared_logarithmic_error: 0.1772\n",
      "Epoch 5/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.2091 - mean_squared_logarithmic_error: 0.2091 - val_loss: 0.1712 - val_mean_squared_logarithmic_error: 0.1712\n",
      "Epoch 6/20\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1978 - mean_squared_logarithmic_error: 0.1978 - val_loss: 0.1632 - val_mean_squared_logarithmic_error: 0.1632\n",
      "Epoch 7/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1903 - mean_squared_logarithmic_error: 0.1904 - val_loss: 0.1589 - val_mean_squared_logarithmic_error: 0.1589\n",
      "Epoch 8/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1888 - mean_squared_logarithmic_error: 0.1888 - val_loss: 0.1501 - val_mean_squared_logarithmic_error: 0.1501\n",
      "Epoch 9/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1854 - mean_squared_logarithmic_error: 0.1854 - val_loss: 0.1584 - val_mean_squared_logarithmic_error: 0.1584\n",
      "Epoch 10/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1829 - mean_squared_logarithmic_error: 0.1830 - val_loss: 0.1842 - val_mean_squared_logarithmic_error: 0.1842\n",
      "Epoch 11/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1786 - mean_squared_logarithmic_error: 0.1786 - val_loss: 0.1478 - val_mean_squared_logarithmic_error: 0.1478\n",
      "Epoch 12/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1786 - mean_squared_logarithmic_error: 0.1785 - val_loss: 0.1503 - val_mean_squared_logarithmic_error: 0.1503\n",
      "Epoch 13/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1716 - mean_squared_logarithmic_error: 0.1716 - val_loss: 0.1456 - val_mean_squared_logarithmic_error: 0.1456\n",
      "Epoch 14/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1732 - mean_squared_logarithmic_error: 0.1732 - val_loss: 0.1490 - val_mean_squared_logarithmic_error: 0.1490\n",
      "Epoch 15/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1665 - mean_squared_logarithmic_error: 0.1665 - val_loss: 0.1411 - val_mean_squared_logarithmic_error: 0.1411\n",
      "Epoch 16/20\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1700 - mean_squared_logarithmic_error: 0.1700 - val_loss: 0.1392 - val_mean_squared_logarithmic_error: 0.1392\n",
      "Epoch 17/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1655 - mean_squared_logarithmic_error: 0.1655 - val_loss: 0.1367 - val_mean_squared_logarithmic_error: 0.1367\n",
      "Epoch 18/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1644 - mean_squared_logarithmic_error: 0.1644 - val_loss: 0.1395 - val_mean_squared_logarithmic_error: 0.1395\n",
      "Epoch 19/20\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1651 - mean_squared_logarithmic_error: 0.1651 - val_loss: 0.1357 - val_mean_squared_logarithmic_error: 0.1357\n",
      "Epoch 20/20\n",
      "320/320 [==============================] - 1s 4ms/step - loss: 0.1632 - mean_squared_logarithmic_error: 0.1631 - val_loss: 0.1312 - val_mean_squared_logarithmic_error: 0.1312\n"
     ]
    }
   ],
   "source": [
    "np_data = full_dataframe\n",
    "X = full_dataframe.iloc[:,:-1]\n",
    "y = full_dataframe.iloc[:,-1]\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y)\n",
    "\n",
    "def scale_datasets(x_train, x_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Standard Scale test and train data\n",
    "    Z - Score normalization\n",
    "    \"\"\"\n",
    "    standard_scaler = StandardScaler()\n",
    "    x_train_scaled = pd.DataFrame(\n",
    "      standard_scaler.fit_transform(x_train),\n",
    "      columns=x_train.columns\n",
    "    )\n",
    "    x_test_scaled = pd.DataFrame(\n",
    "      standard_scaler.transform(x_test),\n",
    "      columns = x_test.columns\n",
    "    )\n",
    "    return x_train_scaled, x_test_scaled\n",
    "\n",
    "x_train_scaled, x_test_scaled = scale_datasets(x_train, x_test)\n",
    "\n",
    "hidden_units1 = 160\n",
    "hidden_units2 = 480\n",
    "hidden_units3 = 256\n",
    "learning_rate = 0.01\n",
    "# Creating model using the Sequential in tensorflow\n",
    "def build_model_using_sequential():\n",
    "    model = Sequential([\n",
    "        Dense(hidden_units1, kernel_initializer='normal', activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_units2, kernel_initializer='normal', activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(1, kernel_initializer='normal', activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "# build the model\n",
    "model = build_model_using_sequential()\n",
    "\n",
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=learning_rate), \n",
    "    metrics=[msle]\n",
    ")\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    x_train_scaled, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# transformer.fit(X_train)\n",
    "# X_train_ = transformer.transform(X_train)\n",
    "# model = LinearRegression().fit(X_train_, y_train)\n",
    "# print(\"score:\", model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/267 [==============================] - 0s 890us/step\n",
      "score: 0.5126582278481012\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test_scaled)\n",
    "count = 0\n",
    "for i in range(len(predictions)):    \n",
    "    delta = y_test.iloc[i] * 0.2\n",
    "    # print(predictions[i], y_test.iloc[i])\n",
    "    if y_test.iloc[i] - delta < predictions[i] and predictions[i] < y_test.iloc[i] + delta:\n",
    "        count += 1\n",
    "        \n",
    "print(\"score:\", count/len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
